{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10846156,"sourceType":"datasetVersion","datasetId":6736009}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Installation","metadata":{}},{"cell_type":"code","source":"packages = [\n    \"langchain\",\n    \"chromadb\",\n    \"gradio\",\n    \"langchain_community\",\n    \"chromadb\",\n    \"InstructorEmbedding==1.0.1\",\n    \"sentence-transformers==2.2.2\",\n    \"transformers>=4.20\",\n    \"datasets>=2.20\",\n    \"pyarrow>=17.0\",\n    \"numpy>=1.0\",\n    \"requests>=2.26\",\n    \"scikit_learn>=1.0.2\",\n    \"scipy>=1.14\",\n    \"torch>=2.0\",\n    \"rich>=13.0\",\n    \"huggingface-hub==0.24.0\",\n    \"protobuf==5.28.2\",\n    \"transformers>=4.20\"\n]\n\n!pip install {\" \".join(packages)}","metadata":{"execution":{"iopub.status.busy":"2025-03-05T03:16:20.213811Z","iopub.execute_input":"2025-03-05T03:16:20.214074Z","iopub.status.idle":"2025-03-05T03:17:06.470505Z","shell.execute_reply.started":"2025-03-05T03:16:20.214051Z","shell.execute_reply":"2025-03-05T03:17:06.469432Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 5.28.2 which is incompatible.\ngoogle-cloud-bigtable 2.27.0 requires google-api-core[grpc]<3.0.0dev,>=2.16.0, but you have google-api-core 1.34.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.2 which is incompatible.\ngoogle-genai 0.2.2 requires websockets<15.0dev,>=13.0, but you have websockets 12.0 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\npandas-gbq 0.25.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npeft 0.14.0 requires huggingface-hub>=0.25.0, but you have huggingface-hub 0.24.0 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\ntensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.2 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\ntensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.28.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install llama-cpp-python==0.2.85 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122","metadata":{"execution":{"iopub.status.busy":"2025-03-05T03:17:06.471490Z","iopub.execute_input":"2025-03-05T03:17:06.471778Z","iopub.status.idle":"2025-03-05T03:17:30.157077Z","shell.execute_reply.started":"2025-03-05T03:17:06.471752Z","shell.execute_reply":"2025-03-05T03:17:30.156036Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\nCollecting llama-cpp-python==0.2.85\n  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.2.85-cu122/llama_cpp_python-0.2.85-cp310-cp310-linux_x86_64.whl (394.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.5/394.5 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.85) (4.12.2)\nRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.85) (1.26.4)\nCollecting diskcache>=5.6.1 (from llama-cpp-python==0.2.85)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.85) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.85) (2.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.2.85) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.2.85) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.2.85) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.2.85) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.2.85) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.2.85) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.2.85) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.2.85) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python==0.2.85) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python==0.2.85) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.20.0->llama-cpp-python==0.2.85) (2024.2.0)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\nSuccessfully installed diskcache-5.6.3 llama-cpp-python-0.2.85\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Package Import","metadata":{}},{"cell_type":"code","source":"import sys\nimport torch\nfrom datetime import datetime\nfrom langchain_community.llms import LlamaCpp\nfrom huggingface_hub import hf_hub_download\nimport gradio as gr\nimport requests\nimport json\nfrom typing import List, Tuple\nfrom InstructorEmbedding import INSTRUCTOR\nfrom chromadb.config import Settings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceInstructEmbeddings","metadata":{"execution":{"iopub.status.busy":"2025-03-05T03:17:30.158130Z","iopub.execute_input":"2025-03-05T03:17:30.158461Z","iopub.status.idle":"2025-03-05T03:17:42.158220Z","shell.execute_reply.started":"2025-03-05T03:17:30.158425Z","shell.execute_reply":"2025-03-05T03:17:42.157517Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"### RAG System Model Loading  \n\nThis RAG system loads an LLM and an embedding model for retrieval-augmented generation. The **LLM (Llama 3.1 8B Instruct)** is downloaded via `hf_hub_download` and configured with an **8096-token context window**, CUDA acceleration (if available), and streaming enabled. The model uses **temperature = 0** for deterministic outputs. The **embedding model (hkunlp/instructor-large)** runs on the selected device (`cuda` or `cpu`) and is initialized by embedding a test query to reduce initial latency. The system ensures efficient retrieval and generation by leveraging **GPU acceleration, batch processing, and optimized token settings**.","metadata":{}},{"cell_type":"code","source":"# LLM Configurations\nCONTEXT_WINDOW_SIZE = 8096\nMAX_NEW_TOKENS = 8096\nN_GPU_LAYERS = 100\nN_BATCH = 512\nN_THREADS = 8\nTEMPERATURE = 0\nVERBOSE = True\nTOP_K = 1\nresume_download = True\nllama_model_id = \"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"\nllama_model_basename = \"Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\"\n\n# Embedding Model Configurations\nembedding_model_id = \"hkunlp/instructor-large\"\n\nmodel_directory = \"models/llm/models\"\n\n\nif torch.cuda.is_available():\n    device_type = \"cuda\"\nelse:\n    device_type = \"cpu\"","metadata":{"execution":{"iopub.status.busy":"2025-03-05T03:17:42.159062Z","iopub.execute_input":"2025-03-05T03:17:42.159667Z","iopub.status.idle":"2025-03-05T03:17:42.213158Z","shell.execute_reply.started":"2025-03-05T03:17:42.159624Z","shell.execute_reply":"2025-03-05T03:17:42.212096Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def load_llamacpp_llm():\n    try:\n        model_path = hf_hub_download(\n            repo_id=llama_model_id,\n            filename=llama_model_basename,\n            resume_download=resume_download,\n            cache_dir=model_directory,\n        )\n        kwargs = {\n            \"model_path\": model_path,\n            \"temperature\": TEMPERATURE,\n            # \"n_threads\": N_THREADS,\n            \"n_ctx\": CONTEXT_WINDOW_SIZE,\n            \"max_tokens\": MAX_NEW_TOKENS,\n            \"verbose\": VERBOSE,\n            \"n_batch\": N_BATCH,\n            \"streaming\": True,\n            # \"top_k\": TOP_K\n        }\n\n        if device_type.lower() == \"cuda\":\n            # set this based on your GPU\n            kwargs[\"n_gpu_layers\"] = N_GPU_LAYERS\n\n        return LlamaCpp(**kwargs)\n    except Exception as e:\n        print(f\"Error occurred while loading LLM: {e}\")\n        return None","metadata":{"execution":{"iopub.status.busy":"2025-03-05T03:17:42.214182Z","iopub.execute_input":"2025-03-05T03:17:42.214545Z","iopub.status.idle":"2025-03-05T03:17:42.562751Z","shell.execute_reply.started":"2025-03-05T03:17:42.214511Z","shell.execute_reply":"2025-03-05T03:17:42.561854Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def build_embedding_model():\n    embeddings = HuggingFaceInstructEmbeddings(\n        model_name=embedding_model_id,\n        model_kwargs={\"device\": device_type},\n    )\n    # Embedding model takes time to load on first query\n    _ = embeddings.embed_query(\"This is a test.\")\n    return embeddings","metadata":{"execution":{"iopub.status.busy":"2025-03-05T03:17:42.565247Z","iopub.execute_input":"2025-03-05T03:17:42.565499Z","iopub.status.idle":"2025-03-05T03:17:42.581769Z","shell.execute_reply.started":"2025-03-05T03:17:42.565477Z","shell.execute_reply":"2025-03-05T03:17:42.580954Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"llm = load_llamacpp_llm()","metadata":{"execution":{"iopub.status.busy":"2025-03-05T03:17:42.585814Z","iopub.execute_input":"2025-03-05T03:17:42.586090Z","iopub.status.idle":"2025-03-05T03:18:10.776301Z","shell.execute_reply.started":"2025-03-05T03:17:42.586069Z","shell.execute_reply":"2025-03-05T03:18:10.775345Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e0c48c81aa5459382a2f979551d5963"}},"metadata":{}},{"name":"stderr","text":"llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from models/llm/models/models--bartowski--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/bf5b95e96dac0462e2a09145ec66cae9a3f12067/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\nllama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\nllama_model_loader: - kv   5:                         general.size_label str              = 8B\nllama_model_loader: - kv   6:                            general.license str              = llama3.1\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\nllama_model_loader: - kv   9:                          llama.block_count u32              = 32\nllama_model_loader: - kv  10:                       llama.context_length u32              = 131072\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 15\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\nllama_model_loader: - kv  28:               general.quantization_version u32              = 2\nllama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\nllama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\nllama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\nllama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\nllama_model_loader: - type  f32:   66 tensors\nllama_model_loader: - type q4_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nllm_load_vocab: special tokens cache size = 256\nllm_load_vocab: token to piece cache size = 0.7999 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 128256\nllm_load_print_meta: n_merges         = 280147\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 131072\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 500000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 8B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 8.03 B\nllm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \nllm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\nllm_load_print_meta: LF token         = 128 'Ä'\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\nllm_load_print_meta: max token length = 256\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:        CPU buffer size =   281.81 MiB\nllm_load_tensors:      CUDA0 buffer size =  4403.50 MiB\n........................................................................................\nllama_new_context_with_model: n_ctx      = 8096\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =  1012.00 MiB\nllama_new_context_with_model: KV self size  = 1012.00 MiB, K (f16):  506.00 MiB, V (f16):  506.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   553.82 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    23.82 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 2\nAVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \nModel metadata: {'quantize.imatrix.entries_count': '224', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.chunks_count': '125', 'quantize.imatrix.file': '/models_out/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.imatrix', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '15', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'general.architecture': 'llama', 'general.basename': 'Meta-Llama-3.1', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Meta Llama 3.1 8B Instruct', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '8B', 'general.license': 'llama3.1', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\nAvailable chat formats from metadata: chat_template.default\nUsing gguf chat template: {{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + '=\"' + arg_val + '\"' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n            {{- '\"parameters\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we're in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n\nUsing chat eos_token: <|eot_id|>\nUsing chat bos_token: <|begin_of_text|>\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"embedding = build_embedding_model()","metadata":{"execution":{"iopub.status.busy":"2025-03-05T03:18:10.777261Z","iopub.execute_input":"2025-03-05T03:18:10.777577Z","iopub.status.idle":"2025-03-05T03:18:33.353491Z","shell.execute_reply.started":"2025-03-05T03:18:10.777544Z","shell.execute_reply":"2025-03-05T03:18:33.352664Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'cached_download' (from 'huggingface_hub.file_download') is deprecated and will be removed from version '0.26'. Use `hf_hub_download` instead.\n  warnings.warn(warning_message, FutureWarning)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d64af0b832aa4bc58ce69ae82eb3cec5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling%2Fconfig.json:   0%|          | 0.00/270 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7122b35b591347acae5f16837eeb9726"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"2_Dense%2Fconfig.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd0ad2cbe7ff4e019890fb602fb7e36e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39b035e79dc244a6803093b96421ecf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/66.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7607d3f9f0264e0e80dd15576f4363e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80989eb71d1a4f9ca0c78ea79d4c40a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd239f6d3ade4545865500fa11831d93"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bb36065a2564ac9b71cb4a67d6827ad"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee97cf9724f04ed2aceb0a4b84fb4850"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5792f585555449918ab463f1704f2452"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e215c4debd140898b46c0686f453d56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54e33dc4d83f433a9f9fe90954b911c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98ecb1437a6b4c52a43f14078b18f62a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6afb5df71388499e94144e72b37f24f0"}},"metadata":{}},{"name":"stdout","text":"load INSTRUCTOR_Transformer\nmax_seq_length  512\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sentence_transformers/models/Dense.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(os.path.join(input_path, 'pytorch_model.bin'), map_location=torch.device('cpu')))\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"def generate(prompt):\n    response = llm.generate([prompt])\n    text = response.flatten()\n    generated_text = text[0].generations[0][0].text\n    return generated_text","metadata":{"execution":{"iopub.status.busy":"2025-03-05T03:18:33.354467Z","iopub.execute_input":"2025-03-05T03:18:33.355359Z","iopub.status.idle":"2025-03-05T03:18:33.360028Z","shell.execute_reply.started":"2025-03-05T03:18:33.355329Z","shell.execute_reply":"2025-03-05T03:18:33.358973Z"},"trusted":true},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def stream_tokens(prompt, max_tokens=300):\n    \"\"\"Generate tokens one by one from the LLM\"\"\"\n    token_count = 0\n\n    try:\n        for text in llm.stream(prompt):\n            yield text\n            token_count += 1\n\n            if token_count >= max_tokens:\n                break\n    except Exception as e:\n        yield f\"\\nError: {str(e)}\"","metadata":{"execution":{"iopub.status.busy":"2025-03-05T03:18:33.361169Z","iopub.execute_input":"2025-03-05T03:18:33.361483Z","iopub.status.idle":"2025-03-05T03:18:33.394117Z","shell.execute_reply.started":"2025-03-05T03:18:33.361453Z","shell.execute_reply":"2025-03-05T03:18:33.393343Z"},"trusted":true},"outputs":[],"execution_count":10},{"cell_type":"code","source":"def stream(prompt):\n    # Safety mechanism to prevent infinite generation\n    max_tokens = 300\n    token_count = 0\n\n    try:\n        for text in llm.stream(prompt):\n            sys.stdout.write(text)\n            sys.stdout.flush()\n\n            # Increment token counter (roughly - each text chunk may contain multiple tokens)\n            token_count += 1\n\n            # Safety check\n            if token_count >= max_tokens:\n                print(\"\\n[Maximum token limit reached]\")\n                break\n\n        print()  # Final newline\n    except Exception as e:\n        print(f\"\\nError during generation: {str(e)}\")","metadata":{"execution":{"iopub.status.busy":"2025-03-05T03:18:33.394847Z","iopub.execute_input":"2025-03-05T03:18:33.395149Z","iopub.status.idle":"2025-03-05T03:18:33.408956Z","shell.execute_reply.started":"2025-03-05T03:18:33.395119Z","shell.execute_reply":"2025-03-05T03:18:33.408101Z"},"scrolled":true,"trusted":true},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Data Ingestion\nIn this section, a .txt file will be processed and chunk will be generated. The embeddings of these chunks will be generated by the embedding model. Then it will be stored in Chromadb vector database.","metadata":{}},{"cell_type":"code","source":"def _create_collection(embedding, dir, settings):\n    db = Chroma(\n        persist_directory=dir,\n        client_settings=settings,\n        embedding_function=embedding,\n        collection_metadata={\n            \"hnsw:space\": \"cosine\",\n            \"hnsw:construction_ef\": 400,\n            \"hnsw:search_ef\": 400,\n            \"hnsw:M\": 128,\n            \"hnsw:resize_factor\": 2.0,\n        },\n    )\n    print(f\"Number of documents in collection: {db._collection.count()}\", flush=True)\n    return db","metadata":{"execution":{"iopub.status.busy":"2025-03-05T03:18:33.429851Z","iopub.execute_input":"2025-03-05T03:18:33.430112Z","iopub.status.idle":"2025-03-05T03:18:33.442574Z","shell.execute_reply.started":"2025-03-05T03:18:33.430079Z","shell.execute_reply":"2025-03-05T03:18:33.441774Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"dir = \"/knowledge_base\"\nsettings = Settings(\n    anonymized_telemetry=False,\n    is_persistent=True,\n    persist_directory=dir,\n)\n\nretriever = _create_collection(embedding, dir, settings)","metadata":{"execution":{"iopub.status.busy":"2025-03-05T03:18:33.443510Z","iopub.execute_input":"2025-03-05T03:18:33.443808Z","iopub.status.idle":"2025-03-05T03:18:34.258040Z","shell.execute_reply.started":"2025-03-05T03:18:33.443786Z","shell.execute_reply":"2025-03-05T03:18:34.256983Z"},"trusted":true},"outputs":[{"name":"stderr","text":"<ipython-input-13-3fc32405d9b1>:2: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n  db = Chroma(\n","output_type":"stream"},{"name":"stdout","text":"Number of documents in collection: 0\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"# Import required libraries\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_core.documents import Document\nimport os\n\n\n# Step 1: Read the .txt file\ndef read_txt_file(file_path):\n    \"\"\"\n    Read the contents of a text file.\n\n    Args:\n        file_path (str): Path to the text file\n\n    Returns:\n        str: Contents of the text file\n    \"\"\"\n    try:\n        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n            return file.read()\n    except Exception as e:\n        print(f\"Error reading file: {e}\")\n        return None\n\n\n# Step 2: Set up text splitter\ndef create_text_splitter(chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Create a RecursiveCharacterTextSplitter.\n\n    Args:\n        chunk_size (int): Maximum size of each text chunk\n        chunk_overlap (int): Number of characters to overlap between chunks\n\n    Returns:\n        RecursiveCharacterTextSplitter: Configured text splitter\n    \"\"\"\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        length_function=len,\n        is_separator_regex=False,\n    )\n    return text_splitter\n\n\n# Step 3: Create documents from text\ndef create_documents(text, text_splitter):\n    \"\"\"\n    Split text into documents using the text splitter.\n\n    Args:\n        text (str): Input text to be split\n        text_splitter (RecursiveCharacterTextSplitter): Text splitter\n\n    Returns:\n        list: List of Document objects\n    \"\"\"\n    # Split the text into chunks\n    texts = text_splitter.split_text(text)\n\n    # Convert text chunks to Document objects\n    documents = [\n        Document(page_content=chunk, metadata={\"source\": \"input_text\"})\n        for chunk in texts\n    ]\n\n    return documents\n\n\n# Step 4: Ingest documents to Chroma vector store\ndef ingest_to_chroma(documents, embeddings, collection_name=\"my_collection\"):\n    \"\"\"\n    Create a Chroma vector store and add documents.\n\n    Args:\n        documents (list): List of Document objects\n        embeddings (Embeddings): Embedding model\n        collection_name (str): Name of the Chroma collection\n\n    Returns:\n        Chroma: Populated Chroma vector store\n    \"\"\"\n    # Add documents to the vector store\n    retriever.add_documents(documents)\n\n    return retriever","metadata":{"execution":{"iopub.status.busy":"2025-03-05T03:18:34.258811Z","iopub.execute_input":"2025-03-05T03:18:34.259116Z","iopub.status.idle":"2025-03-05T03:18:34.270578Z","shell.execute_reply.started":"2025-03-05T03:18:34.259092Z","shell.execute_reply":"2025-03-05T03:18:34.269773Z"},"trusted":true},"outputs":[],"execution_count":15},{"cell_type":"code","source":"file_path = \"/kaggle/input/rag-documents/About UIU.txt\"\n\n# Read the text file\ntext = read_txt_file(file_path)\n\nif not text:\n    print(f\"No text is found!\")\nelse:\n    # Create text splitter\n    text_splitter = create_text_splitter()\n\n    # Create documents\n    documents = create_documents(text, text_splitter)\n\n    # Ingest to Chroma vector store\n    retriever = ingest_to_chroma(documents, embedding)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T03:18:34.271484Z","iopub.execute_input":"2025-03-05T03:18:34.271800Z","iopub.status.idle":"2025-03-05T03:18:34.515990Z","shell.execute_reply.started":"2025-03-05T03:18:34.271772Z","shell.execute_reply":"2025-03-05T03:18:34.515174Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"# Test the retriever\nquery = \"Tell me about UIU\"\ndocs = retriever.similarity_search_with_relevance_scores(query=query)\ncontent, _ = docs[0]\nprint(content.page_content)","metadata":{"execution":{"iopub.status.busy":"2025-03-05T03:23:21.066958Z","iopub.execute_input":"2025-03-05T03:23:21.067280Z","iopub.status.idle":"2025-03-05T03:23:21.105229Z","shell.execute_reply.started":"2025-03-05T03:23:21.067256Z","shell.execute_reply":"2025-03-05T03:23:21.104535Z"},"trusted":true},"outputs":[{"name":"stdout","text":"United International University (Bengali: ইউনাইটেড ইন্টারন্যাশনাল ইউনিভার্সিটি, also known as UIU) is a private research university in Dhaka, Bangladesh.[2]\n\nThe government of Bangladesh approved the establishment of United International University in 2003.[3] United International University was established with the generous support and patronage of the United Group.[4]\n\nIn 2024, according to the QS World University Rankings, United International University (UIU) is ranked in 1201-1400 globally, making it the 3rd highest-ranking private university in Bangladesh, alongside Daffodil International University (DIU) and East West University (EWU).[5]\n\n\nCampus Desciption:\nThe permanent campus is on a 25-bigha (8.25 acre) plot of land, located at the 'United City' at Satarkul, Badda (1.5 km east of Embassy of the United States, Dhaka), adjacent to Madani Avenue.[6]\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"## Prompts\n\nPrompts of this RAG chatbot. For this chatbot two prompts are necessary:\n- **Standalone Query Generation Prompt:** The followup user query will be regenrated based on the previous conversation history\n- **RAG Chat Prompt:** This prompt will be used to generate final response of this RAG chatbot.","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\n\n\ndef get_chat_prompt(user_input, history, context=None):\n    today_date = datetime.today().strftime(\"%d %B %Y\")  # Dynamic date insertion\n\n    prompt = (\n        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n        f\"Cutting Knowledge Date: December 2023\\n\"\n        f\"Today Date: {today_date}\\n\\n\"\n        \"You are a helpful assistant. DO NOT provide information which is not present on the Retrieved Context.\\n\"\n    )\n\n    # Add retrieved context if available\n    if context:\n        prompt += \"\\nRetrieved Context:\\n\" + context + \"\\n\"\n\n    prompt += \"<|eot_id|>\"\n\n    # Append chat history\n    for role, message in history:\n        prompt += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{message}<|eot_id|>\"\n\n    # Append current user input\n    prompt += f\"<|start_header_id|>user<|end_header_id|>\\n\\n{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n    return prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T03:23:21.106192Z","iopub.execute_input":"2025-03-05T03:23:21.106415Z","iopub.status.idle":"2025-03-05T03:23:21.110761Z","shell.execute_reply.started":"2025-03-05T03:23:21.106396Z","shell.execute_reply":"2025-03-05T03:23:21.109930Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"\ndef get_standalone_query_generation_prompt(user_input, history):\n    today_date = datetime.today().strftime(\"%d %B %Y\")  # Dynamic date insertion\n\n    prompt = (\n        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n        f\"Cutting Knowledge Date: December 2023\\n\"\n        f\"Today Date: {today_date}\\n\\n\"\n        \"You are a helpful assistant. Write the standalone query of the last user message so that it contains all the information of this question and best suited for context retrieval. Just write the query in detailed form. DO NOT write any extra explanation.\\n\"\n    )\n\n    prompt += \"<|eot_id|>\"\n\n    # Append chat history\n    for role, message in history:\n        prompt += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{message}<|eot_id|>\"\n\n    # Append current user input\n    prompt += f\"<|start_header_id|>user<|end_header_id|>\\n\\n{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nStandalone Query: \"\n\n    return prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T03:23:21.112442Z","iopub.execute_input":"2025-03-05T03:23:21.112636Z","iopub.status.idle":"2025-03-05T03:23:21.129659Z","shell.execute_reply.started":"2025-03-05T03:23:21.112618Z","shell.execute_reply":"2025-03-05T03:23:21.128996Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## ChatSystem\n\nThe **ChatSystem** class integrates an LLM with a retriever to enhance chatbot responses using retrieved context. It maintains conversation history and refines user queries into standalone questions before retrieval. The `retrieve` method fetches relevant documents, while `chat` generates responses based on context and history. Responses are logged and appended to history for continuity. This system ensures more accurate and context-aware chatbot interactions.","metadata":{}},{"cell_type":"code","source":"class ChatSystem:\n    def __init__(self, llm, retriever):\n        self.history = []\n        self.llm = llm\n        self.retriever = retriever\n\n    def retrieve(self, query):\n        docs = self.retriever.similarity_search_with_relevance_scores(query=query)\n        return \"\".join(content.page_content for content, _ in docs)\n\n    def chat(self, user_input):\n        standalone_query_prompt = get_standalone_query_generation_prompt(user_input, self.history)\n        standalone_query = generate(standalone_query_prompt)\n        print(f'Standalone Query: {standalone_query}')\n\n        context = self.retrieve(standalone_query)\n        prompt = get_chat_prompt(user_input, self.history, context)\n        response = generate(prompt)\n\n        print(f\"User: \\n{user_input} \\nAssistant:\\n{response}\\n\", flush=True)\n        print(\"-\" * 100, flush=True)\n\n        self.history.append((\"user\", user_input))\n        self.history.append((\"assistant\", response))\n        return response","metadata":{"execution":{"iopub.status.busy":"2025-03-05T03:39:59.129750Z","iopub.execute_input":"2025-03-05T03:39:59.130117Z","iopub.status.idle":"2025-03-05T03:39:59.136469Z","shell.execute_reply.started":"2025-03-05T03:39:59.130090Z","shell.execute_reply":"2025-03-05T03:39:59.135598Z"},"trusted":true},"outputs":[],"execution_count":30},{"cell_type":"code","source":"chatsystem = ChatSystem(llm, retriever)\n\nmessage = \"Tell about the vc of UIU\"\nchatsystem.chat(message)","metadata":{"execution":{"iopub.status.busy":"2025-03-05T03:40:01.653387Z","iopub.execute_input":"2025-03-05T03:40:01.653689Z","iopub.status.idle":"2025-03-05T03:40:05.191994Z","shell.execute_reply.started":"2025-03-05T03:40:01.653667Z","shell.execute_reply":"2025-03-05T03:40:05.191293Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:1129: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1091.02 ms\nllama_print_timings:      sample time =      40.91 ms /    20 runs   (    2.05 ms per token,   488.82 tokens per second)\nllama_print_timings: prompt eval time =     309.07 ms /    63 tokens (    4.91 ms per token,   203.84 tokens per second)\nllama_print_timings:        eval time =     499.93 ms /    19 runs   (   26.31 ms per token,    38.01 tokens per second)\nllama_print_timings:       total time =     867.38 ms /    82 tokens\n/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:1129: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\nLlama.generate: prefix-match hit\n","output_type":"stream"},{"name":"stdout","text":"Standalone Query:  What is the vision and mission statement of University of Information Technology (UIU) Vice Chancellor?\n","output_type":"stream"},{"name":"stderr","text":"\nllama_print_timings:        load time =    1091.02 ms\nllama_print_timings:      sample time =      43.70 ms /    22 runs   (    1.99 ms per token,   503.42 tokens per second)\nllama_print_timings: prompt eval time =    1946.80 ms /   885 tokens (    2.20 ms per token,   454.59 tokens per second)\nllama_print_timings:        eval time =     608.70 ms /    21 runs   (   28.99 ms per token,    34.50 tokens per second)\nllama_print_timings:       total time =    2619.55 ms /   906 tokens\n","output_type":"stream"},{"name":"stdout","text":"User: \nTell about the vc of UIU \nAssistant:\nThe Vice-Chancellor of United International University (UIU) is Md. Abul Kashem Mia.\n\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"'The Vice-Chancellor of United International University (UIU) is Md. Abul Kashem Mia.'"},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"## Gradio Chatbot Interface  \n\nThis Gradio-based chatbot interface integrates an LLM-powered **ChatSystem** for real-time conversations. The `chatbot` function processes user messages, maintains conversation history, and generates responses. The interface includes a **chat history panel**, a **textbox for user input**, and a **clear chat button**. The chatbot runs inside a `Blocks` layout with Markdown headers for branding. The application is launched with `demo.launch(share=True)`, allowing external access.","metadata":{}},{"cell_type":"code","source":"def chatbot(message: str, history: List[Tuple[str, str]], chatsystem: ChatSystem) -> List[Tuple[str, str]]:\n    \"\"\"Processes messages and maintains conversation history for each session.\"\"\"\n    bot_response = chatsystem.chat(message)\n    history.append((message, bot_response))\n    return history\n\n# Create the Gradio interface\nwith gr.Blocks(css=\"footer {visibility: hidden}\") as demo:\n    gr.Markdown(\"# 🤖 RAG Chatbot\")\n    gr.Markdown(\"Chat with this LLM-powered bot! Ask any question and get an intelligent response.\")\n    gr.Markdown(\"Powered by Nascenia\")\n\n    chatbot_interface = gr.Chatbot(label=\"Chat History\", height=600)\n    msg = gr.Textbox(label=\"Type your message\", placeholder=\"Type your message here...\", lines=1)\n    clear = gr.Button(\"Clear Chat\")\n\n    chatsystem_state = gr.State(lambda: ChatSystem(llm, retriever))  # Each user gets a separate ChatSystem\n\n    def user_input(message, history, chatsystem):\n        history = history or []\n        return \"\", chatbot(message, history, chatsystem)\n\n    def clear_chat():\n        return None, None, ChatSystem(llm, retriever)  # Reset the chat system for a new session\n\n    msg.submit(user_input, [msg, chatbot_interface, chatsystem_state], [msg, chatbot_interface])\n    clear.click(clear_chat, None, [msg, chatbot_interface, chatsystem_state])\n\nif __name__ == \"__main__\":\n    demo.launch(share=True)\n","metadata":{"execution":{"iopub.status.busy":"2025-03-05T03:43:57.174956Z","iopub.execute_input":"2025-03-05T03:43:57.175278Z","iopub.status.idle":"2025-03-05T03:43:59.695729Z","shell.execute_reply.started":"2025-03-05T03:43:57.175255Z","shell.execute_reply":"2025-03-05T03:43:59.695074Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","text":"Running on local URL:  http://127.0.0.1:7863\nRunning on public URL: https://979d50604ffa3dae28.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://979d50604ffa3dae28.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":33}]}