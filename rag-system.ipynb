{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10846156,"sourceType":"datasetVersion","datasetId":6736009}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Installation","metadata":{}},{"cell_type":"code","source":"packages = [\n    \"langchain\",\n    \"chromadb\",\n    \"gradio\",\n    \"langchain_community\",\n    \"chromadb\",\n    \"InstructorEmbedding==1.0.1\",\n    \"sentence-transformers==2.2.2\",\n    \"transformers>=4.20\",\n    \"datasets>=2.20\",\n    \"pyarrow>=17.0\",\n    \"numpy>=1.0\",\n    \"requests>=2.26\",\n    \"scikit_learn>=1.0.2\",\n    \"scipy>=1.14\",\n    \"torch>=2.0\",\n    \"rich>=13.0\",\n    \"huggingface-hub==0.24.0\",\n    \"protobuf==5.28.2\",\n    \"transformers>=4.20\"\n]\n\n!pip install {\" \".join(packages)}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:19:36.313950Z","iopub.execute_input":"2025-02-25T07:19:36.314263Z","iopub.status.idle":"2025-02-25T07:20:10.087296Z","shell.execute_reply.started":"2025-02-25T07:19:36.314241Z","shell.execute_reply":"2025-02-25T07:20:10.086487Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\ngoogle-genai 0.2.2 requires websockets<15.0dev,>=13.0, but you have websockets 12.0 which is incompatible.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nplotnine 0.14.4 requires matplotlib>=3.8.0, but you have matplotlib 3.7.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install llama-cpp-python==0.2.85 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:20:10.088555Z","iopub.execute_input":"2025-02-25T07:20:10.088808Z","iopub.status.idle":"2025-02-25T07:20:13.696927Z","shell.execute_reply.started":"2025-02-25T07:20:10.088784Z","shell.execute_reply":"2025-02-25T07:20:13.695950Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\nRequirement already satisfied: llama-cpp-python==0.2.85 in /usr/local/lib/python3.10/dist-packages (0.2.85)\nRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.85) (4.12.2)\nRequirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.85) (1.26.4)\nRequirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.85) (5.6.3)\nRequirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.85) (3.1.4)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.85) (2.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.2.85) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.2.85) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.2.85) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.2.85) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.2.85) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.20.0->llama-cpp-python==0.2.85) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.2.85) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python==0.2.85) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python==0.2.85) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python==0.2.85) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.20.0->llama-cpp-python==0.2.85) (2024.2.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"## Package Import","metadata":{}},{"cell_type":"code","source":"import sys\nimport torch\nfrom datetime import datetime\nfrom langchain_community.llms import LlamaCpp\nfrom huggingface_hub import hf_hub_download\nimport gradio as gr\nimport requests\nimport json\nfrom typing import List, Tuple\nfrom InstructorEmbedding import INSTRUCTOR\nfrom chromadb.config import Settings\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceInstructEmbeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:28:36.783478Z","iopub.execute_input":"2025-02-25T07:28:36.783822Z","iopub.status.idle":"2025-02-25T07:28:36.806447Z","shell.execute_reply.started":"2025-02-25T07:28:36.783799Z","shell.execute_reply":"2025-02-25T07:28:36.805485Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"## Load Models","metadata":{}},{"cell_type":"code","source":"# LLM Configurations\nCONTEXT_WINDOW_SIZE = 8096\nMAX_NEW_TOKENS = 8096\nN_GPU_LAYERS = 100\nN_BATCH = 512\nN_THREADS = 8\nTEMPERATURE = 0\nVERBOSE = True\nTOP_K = 1\nresume_download = True\nllama_model_id = \"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"\nllama_model_basename = \"Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf\"\n\n# Embedding Model Configurations\nembedding_model_id = \"hkunlp/instructor-large\"\n\nmodel_directory = \"models/llm/models\"\n\n\nif torch.cuda.is_available():\n    device_type = \"cuda\"\nelse:\n    device_type = \"cpu\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:20:22.630814Z","iopub.execute_input":"2025-02-25T07:20:22.631332Z","iopub.status.idle":"2025-02-25T07:20:22.689017Z","shell.execute_reply.started":"2025-02-25T07:20:22.631301Z","shell.execute_reply":"2025-02-25T07:20:22.688044Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def load_llamacpp_llm():\n    try:\n        model_path = hf_hub_download(\n            repo_id=llama_model_id,\n            filename=llama_model_basename,\n            resume_download=resume_download,\n            cache_dir=model_directory,\n        )\n        kwargs = {\n            \"model_path\": model_path,\n            \"temperature\": TEMPERATURE,\n            # \"n_threads\": N_THREADS,\n            \"n_ctx\": CONTEXT_WINDOW_SIZE,\n            \"max_tokens\": MAX_NEW_TOKENS,\n            \"verbose\": VERBOSE,\n            \"n_batch\": N_BATCH,\n            \"streaming\": True\n            # \"top_k\": TOP_K\n        }\n\n        if device_type.lower() == \"cuda\":\n            # set this based on your GPU\n            kwargs[\"n_gpu_layers\"] = N_GPU_LAYERS\n\n        return LlamaCpp(**kwargs)\n    except Exception as e:\n        print(f\"Error occurred while loading LLM: {e}\")\n        return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:20:22.690045Z","iopub.execute_input":"2025-02-25T07:20:22.690378Z","iopub.status.idle":"2025-02-25T07:20:22.729361Z","shell.execute_reply.started":"2025-02-25T07:20:22.690345Z","shell.execute_reply":"2025-02-25T07:20:22.728440Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def build_embedding_model():\n    embeddings = HuggingFaceInstructEmbeddings(\n        model_name=embedding_model_id,\n        model_kwargs={\"device\": device_type},\n    )\n    # Embedding model takes time to load on first query\n    _ = embeddings.embed_query(\"This is a test.\")\n    return embeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:20:22.730169Z","iopub.execute_input":"2025-02-25T07:20:22.730431Z","iopub.status.idle":"2025-02-25T07:20:22.745218Z","shell.execute_reply.started":"2025-02-25T07:20:22.730378Z","shell.execute_reply":"2025-02-25T07:20:22.744295Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"llm = load_llamacpp_llm()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:20:22.746252Z","iopub.execute_input":"2025-02-25T07:20:22.746569Z","iopub.status.idle":"2025-02-25T07:22:24.364082Z","shell.execute_reply.started":"2025-02-25T07:20:22.746541Z","shell.execute_reply":"2025-02-25T07:22:24.363217Z"},"scrolled":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e298b7efd25e46b59ef7ec5906fd1351"}},"metadata":{}},{"name":"stderr","text":"llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from models/llm/models/models--bartowski--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/bf5b95e96dac0462e2a09145ec66cae9a3f12067/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf (version GGUF V3 (latest))\nllama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\nllama_model_loader: - kv   0:                       general.architecture str              = llama\nllama_model_loader: - kv   1:                               general.type str              = model\nllama_model_loader: - kv   2:                               general.name str              = Meta Llama 3.1 8B Instruct\nllama_model_loader: - kv   3:                           general.finetune str              = Instruct\nllama_model_loader: - kv   4:                           general.basename str              = Meta-Llama-3.1\nllama_model_loader: - kv   5:                         general.size_label str              = 8B\nllama_model_loader: - kv   6:                            general.license str              = llama3.1\nllama_model_loader: - kv   7:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\nllama_model_loader: - kv   8:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\nllama_model_loader: - kv   9:                          llama.block_count u32              = 32\nllama_model_loader: - kv  10:                       llama.context_length u32              = 131072\nllama_model_loader: - kv  11:                     llama.embedding_length u32              = 4096\nllama_model_loader: - kv  12:                  llama.feed_forward_length u32              = 14336\nllama_model_loader: - kv  13:                 llama.attention.head_count u32              = 32\nllama_model_loader: - kv  14:              llama.attention.head_count_kv u32              = 8\nllama_model_loader: - kv  15:                       llama.rope.freq_base f32              = 500000.000000\nllama_model_loader: - kv  16:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\nllama_model_loader: - kv  17:                          general.file_type u32              = 15\nllama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\nllama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\nllama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\nllama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\nllama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\nllama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\nllama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\nllama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\nllama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\nllama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\nllama_model_loader: - kv  28:               general.quantization_version u32              = 2\nllama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Meta-Llama-3.1-8B-Instruc...\nllama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\nllama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 224\nllama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\nllama_model_loader: - type  f32:   66 tensors\nllama_model_loader: - type q4_K:  193 tensors\nllama_model_loader: - type q6_K:   33 tensors\nllm_load_vocab: special tokens cache size = 256\nllm_load_vocab: token to piece cache size = 0.7999 MB\nllm_load_print_meta: format           = GGUF V3 (latest)\nllm_load_print_meta: arch             = llama\nllm_load_print_meta: vocab type       = BPE\nllm_load_print_meta: n_vocab          = 128256\nllm_load_print_meta: n_merges         = 280147\nllm_load_print_meta: vocab_only       = 0\nllm_load_print_meta: n_ctx_train      = 131072\nllm_load_print_meta: n_embd           = 4096\nllm_load_print_meta: n_layer          = 32\nllm_load_print_meta: n_head           = 32\nllm_load_print_meta: n_head_kv        = 8\nllm_load_print_meta: n_rot            = 128\nllm_load_print_meta: n_swa            = 0\nllm_load_print_meta: n_embd_head_k    = 128\nllm_load_print_meta: n_embd_head_v    = 128\nllm_load_print_meta: n_gqa            = 4\nllm_load_print_meta: n_embd_k_gqa     = 1024\nllm_load_print_meta: n_embd_v_gqa     = 1024\nllm_load_print_meta: f_norm_eps       = 0.0e+00\nllm_load_print_meta: f_norm_rms_eps   = 1.0e-05\nllm_load_print_meta: f_clamp_kqv      = 0.0e+00\nllm_load_print_meta: f_max_alibi_bias = 0.0e+00\nllm_load_print_meta: f_logit_scale    = 0.0e+00\nllm_load_print_meta: n_ff             = 14336\nllm_load_print_meta: n_expert         = 0\nllm_load_print_meta: n_expert_used    = 0\nllm_load_print_meta: causal attn      = 1\nllm_load_print_meta: pooling type     = 0\nllm_load_print_meta: rope type        = 0\nllm_load_print_meta: rope scaling     = linear\nllm_load_print_meta: freq_base_train  = 500000.0\nllm_load_print_meta: freq_scale_train = 1\nllm_load_print_meta: n_ctx_orig_yarn  = 131072\nllm_load_print_meta: rope_finetuned   = unknown\nllm_load_print_meta: ssm_d_conv       = 0\nllm_load_print_meta: ssm_d_inner      = 0\nllm_load_print_meta: ssm_d_state      = 0\nllm_load_print_meta: ssm_dt_rank      = 0\nllm_load_print_meta: model type       = 8B\nllm_load_print_meta: model ftype      = Q4_K - Medium\nllm_load_print_meta: model params     = 8.03 B\nllm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \nllm_load_print_meta: general.name     = Meta Llama 3.1 8B Instruct\nllm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\nllm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\nllm_load_print_meta: LF token         = 128 'Ä'\nllm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\nllm_load_print_meta: max token length = 256\nggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\nggml_cuda_init: found 1 CUDA devices:\n  Device 0: Tesla P100-PCIE-16GB, compute capability 6.0, VMM: yes\nllm_load_tensors: ggml ctx size =    0.27 MiB\nllm_load_tensors: offloading 32 repeating layers to GPU\nllm_load_tensors: offloading non-repeating layers to GPU\nllm_load_tensors: offloaded 33/33 layers to GPU\nllm_load_tensors:        CPU buffer size =   281.81 MiB\nllm_load_tensors:      CUDA0 buffer size =  4403.50 MiB\n........................................................................................\nllama_new_context_with_model: n_ctx      = 8096\nllama_new_context_with_model: n_batch    = 512\nllama_new_context_with_model: n_ubatch   = 512\nllama_new_context_with_model: flash_attn = 0\nllama_new_context_with_model: freq_base  = 10000.0\nllama_new_context_with_model: freq_scale = 1\nllama_kv_cache_init:      CUDA0 KV buffer size =  1012.00 MiB\nllama_new_context_with_model: KV self size  = 1012.00 MiB, K (f16):  506.00 MiB, V (f16):  506.00 MiB\nllama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\nllama_new_context_with_model:      CUDA0 compute buffer size =   553.82 MiB\nllama_new_context_with_model:  CUDA_Host compute buffer size =    23.82 MiB\nllama_new_context_with_model: graph nodes  = 1030\nllama_new_context_with_model: graph splits = 2\nAVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \nModel metadata: {'quantize.imatrix.entries_count': '224', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.chunks_count': '125', 'quantize.imatrix.file': '/models_out/Meta-Llama-3.1-8B-Instruct-GGUF/Meta-Llama-3.1-8B-Instruct.imatrix', 'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'llama.rope.dimension_count': '128', 'llama.vocab_size': '128256', 'general.file_type': '15', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '500000.000000', 'general.architecture': 'llama', 'general.basename': 'Meta-Llama-3.1', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '131072', 'general.name': 'Meta Llama 3.1 8B Instruct', 'general.finetune': 'Instruct', 'general.type': 'model', 'general.size_label': '8B', 'general.license': 'llama3.1', 'llama.feed_forward_length': '14336', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\nAvailable chat formats from metadata: chat_template.default\nUsing gguf chat template: {{- bos_token }}\n{%- if custom_tools is defined %}\n    {%- set tools = custom_tools %}\n{%- endif %}\n{%- if not tools_in_user_message is defined %}\n    {%- set tools_in_user_message = true %}\n{%- endif %}\n{%- if not date_string is defined %}\n    {%- set date_string = \"26 Jul 2024\" %}\n{%- endif %}\n{%- if not tools is defined %}\n    {%- set tools = none %}\n{%- endif %}\n\n{#- This block extracts the system message, so we can slot it into the right place. #}\n{%- if messages[0]['role'] == 'system' %}\n    {%- set system_message = messages[0]['content']|trim %}\n    {%- set messages = messages[1:] %}\n{%- else %}\n    {%- set system_message = \"\" %}\n{%- endif %}\n\n{#- System message + builtin tools #}\n{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n{%- if builtin_tools is defined or tools is not none %}\n    {{- \"Environment: ipython\\n\" }}\n{%- endif %}\n{%- if builtin_tools is defined %}\n    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n{%- endif %}\n{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n{%- if tools is not none and not tools_in_user_message %}\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n{%- endif %}\n{{- system_message }}\n{{- \"<|eot_id|>\" }}\n\n{#- Custom tools are passed in a user message with some extra guidance #}\n{%- if tools_in_user_message and not tools is none %}\n    {#- Extract the first user message so we can plug it in here #}\n    {%- if messages | length != 0 %}\n        {%- set first_user_message = messages[0]['content']|trim %}\n        {%- set messages = messages[1:] %}\n    {%- else %}\n        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n{%- endif %}\n    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n    {{- \"Do not use variables.\\n\\n\" }}\n    {%- for t in tools %}\n        {{- t | tojson(indent=4) }}\n        {{- \"\\n\\n\" }}\n    {%- endfor %}\n    {{- first_user_message + \"<|eot_id|>\"}}\n{%- endif %}\n\n{%- for message in messages %}\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n    {%- elif 'tool_calls' in message %}\n        {%- if not message.tool_calls|length == 1 %}\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n        {%- endif %}\n        {%- set tool_call = message.tool_calls[0].function %}\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\n                {{- arg_name + '=\"' + arg_val + '\"' }}\n                {%- if not loop.last %}\n                    {{- \", \" }}\n                {%- endif %}\n                {%- endfor %}\n            {{- \")\" }}\n        {%- else  %}\n            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n            {{- '\"parameters\": ' }}\n            {{- tool_call.arguments | tojson }}\n            {{- \"}\" }}\n        {%- endif %}\n        {%- if builtin_tools is defined %}\n            {#- This means we're in ipython mode #}\n            {{- \"<|eom_id|>\" }}\n        {%- else %}\n            {{- \"<|eot_id|>\" }}\n        {%- endif %}\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n        {%- if message.content is mapping or message.content is iterable %}\n            {{- message.content | tojson }}\n        {%- else %}\n            {{- message.content }}\n        {%- endif %}\n        {{- \"<|eot_id|>\" }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n{%- endif %}\n\nUsing chat eos_token: <|eot_id|>\nUsing chat bos_token: <|begin_of_text|>\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"embedding = build_embedding_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:22:24.365047Z","iopub.execute_input":"2025-02-25T07:22:24.365366Z","iopub.status.idle":"2025-02-25T07:22:48.945349Z","shell.execute_reply.started":"2025-02-25T07:22:24.365336Z","shell.execute_reply":"2025-02-25T07:22:48.944404Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:131: FutureWarning: 'cached_download' (from 'huggingface_hub.file_download') is deprecated and will be removed from version '0.26'. Use `hf_hub_download` instead.\n  warnings.warn(warning_message, FutureWarning)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":".gitattributes:   0%|          | 0.00/1.48k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d22a844f569488d818d18d545c66db5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1_Pooling%2Fconfig.json:   0%|          | 0.00/270 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d58adb7e100483b9674ff8d72e1cda8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"2_Dense%2Fconfig.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce2313a0f8ac4d579561ebb690ae7836"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/3.15M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7ebcd36816e4771b54fddddc54e5297"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/66.3k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2356fb50d7d84f239f87b8ae2ce9f15f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.53k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afaf43eef0ce436784eba0c147ae3d0d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2a8f439fe394945a1abddffa9e46021"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.34G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ff3e86a78864ca0a24748265f6fa336"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc06e4b1eeac4bdc83e151eddf094e3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"438b400008044c14b6b66ba6acad6178"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"079fc1ed97ab476eb9975213d18014ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b5aca72a8aa442f9f0bbf1891e19d24"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1b2fc461af0642a1a2bfdc4344be31b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/461 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a417bfb83d14353a786de2da114019c"}},"metadata":{}},{"name":"stdout","text":"load INSTRUCTOR_Transformer\nmax_seq_length  512\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sentence_transformers/models/Dense.py:63: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(os.path.join(input_path, 'pytorch_model.bin'), map_location=torch.device('cpu')))\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"def generate(prompt):\n    response = llm.generate([prompt])\n    text = response.flatten()\n    generated_text = text[0].generations[0][0].text\n    return generated_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:22:48.947663Z","iopub.execute_input":"2025-02-25T07:22:48.948326Z","iopub.status.idle":"2025-02-25T07:22:48.952401Z","shell.execute_reply.started":"2025-02-25T07:22:48.948301Z","shell.execute_reply":"2025-02-25T07:22:48.951497Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def stream_tokens(prompt, max_tokens=300):\n    \"\"\"Generate tokens one by one from the LLM\"\"\"\n    token_count = 0\n    \n    try:\n        for text in llm.stream(prompt):\n            yield text\n            token_count += 1\n            \n            if token_count >= max_tokens:\n                break\n    except Exception as e:\n        yield f\"\\nError: {str(e)}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:22:48.953563Z","iopub.execute_input":"2025-02-25T07:22:48.953844Z","iopub.status.idle":"2025-02-25T07:22:48.986870Z","shell.execute_reply.started":"2025-02-25T07:22:48.953812Z","shell.execute_reply":"2025-02-25T07:22:48.985936Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"\ndef stream(prompt):\n    # Safety mechanism to prevent infinite generation\n    max_tokens = 300\n    token_count = 0\n    \n    try:\n        for text in llm.stream(prompt):\n            sys.stdout.write(text)\n            sys.stdout.flush()\n            \n            # Increment token counter (roughly - each text chunk may contain multiple tokens)\n            token_count += 1\n            \n            # Safety check\n            if token_count >= max_tokens:\n                print(\"\\n[Maximum token limit reached]\")\n                break\n                \n        print()  # Final newline\n    except Exception as e:\n        print(f\"\\nError during generation: {str(e)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:22:48.987859Z","iopub.execute_input":"2025-02-25T07:22:48.988174Z","iopub.status.idle":"2025-02-25T07:22:49.006580Z","shell.execute_reply.started":"2025-02-25T07:22:48.988148Z","shell.execute_reply":"2025-02-25T07:22:49.005813Z"},"scrolled":true},"outputs":[],"execution_count":14},{"cell_type":"code","source":"prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 23 July 2024\n\nYou are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nWhat is AI?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n\nprint(\"Tokens generated one by one:\")\nfor i, token in enumerate(stream_tokens(llm, prompt)):\n    print(f\"Token {i}: {repr(token)}\")  # repr() shows whitespace characters clearly\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:22:49.007574Z","iopub.execute_input":"2025-02-25T07:22:49.007817Z","iopub.status.idle":"2025-02-25T07:22:49.023251Z","shell.execute_reply.started":"2025-02-25T07:22:49.007796Z","shell.execute_reply":"2025-02-25T07:22:49.022477Z"}},"outputs":[{"name":"stdout","text":"Tokens generated one by one:\nToken 0: \"\\nError: Invalid input type <class 'langchain_community.llms.llamacpp.LlamaCpp'>. Must be a PromptValue, str, or list of BaseMessages.\"\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Data Ingestion","metadata":{}},{"cell_type":"code","source":"def _create_collection(embedding, dir, settings):\n        db = Chroma(\n            persist_directory=dir,\n            client_settings=settings,\n            embedding_function=embedding,\n            collection_metadata={\n                \"hnsw:space\": \"cosine\",\n                \"hnsw:construction_ef\": 400,\n                \"hnsw:search_ef\": 400,\n                \"hnsw:M\": 128,\n                \"hnsw:resize_factor\": 2.0,\n            }\n        )        \n        print(f\"Number of documents in collection: {db._collection.count()}\", flush=True)\n        return db","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:29:15.798328Z","iopub.execute_input":"2025-02-25T07:29:15.798707Z","iopub.status.idle":"2025-02-25T07:29:15.803288Z","shell.execute_reply.started":"2025-02-25T07:29:15.798678Z","shell.execute_reply":"2025-02-25T07:29:15.802293Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"dir = '/knowledge_base'\nsettings = Settings(\n            anonymized_telemetry=False,\n            is_persistent=True,\n            persist_directory=dir,\n        )\n\nretriever = _create_collection(embedding, dir, settings)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:29:18.290509Z","iopub.execute_input":"2025-02-25T07:29:18.290844Z","iopub.status.idle":"2025-02-25T07:29:18.304589Z","shell.execute_reply.started":"2025-02-25T07:29:18.290818Z","shell.execute_reply":"2025-02-25T07:29:18.303689Z"}},"outputs":[{"name":"stdout","text":"Number of documents in collection: 0\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# Import required libraries\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_core.documents import Document\nimport os\n\n# Step 1: Read the .txt file\ndef read_txt_file(file_path):\n    \"\"\"\n    Read the contents of a text file.\n    \n    Args:\n        file_path (str): Path to the text file\n    \n    Returns:\n        str: Contents of the text file\n    \"\"\"\n    try:\n        with open(file_path, 'r', encoding='utf-8') as file:\n            return file.read()\n    except Exception as e:\n        print(f\"Error reading file: {e}\")\n        return None\n\n# Step 2: Set up text splitter\ndef create_text_splitter(chunk_size=1000, chunk_overlap=200):\n    \"\"\"\n    Create a RecursiveCharacterTextSplitter.\n    \n    Args:\n        chunk_size (int): Maximum size of each text chunk\n        chunk_overlap (int): Number of characters to overlap between chunks\n    \n    Returns:\n        RecursiveCharacterTextSplitter: Configured text splitter\n    \"\"\"\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=chunk_size,\n        chunk_overlap=chunk_overlap,\n        length_function=len,\n        is_separator_regex=False\n    )\n    return text_splitter\n\n# Step 3: Create documents from text\ndef create_documents(text, text_splitter):\n    \"\"\"\n    Split text into documents using the text splitter.\n    \n    Args:\n        text (str): Input text to be split\n        text_splitter (RecursiveCharacterTextSplitter): Text splitter\n    \n    Returns:\n        list: List of Document objects\n    \"\"\"\n    # Split the text into chunks\n    texts = text_splitter.split_text(text)\n    \n    # Convert text chunks to Document objects\n    documents = [\n        Document(\n            page_content=chunk, \n            metadata={\"source\": \"input_text\"}\n        ) for chunk in texts\n    ]\n    \n    return documents\n\n# Step 4: Ingest documents to Chroma vector store\ndef ingest_to_chroma(documents, embeddings, collection_name=\"my_collection\"):\n    \"\"\"\n    Create a Chroma vector store and add documents.\n    \n    Args:\n        documents (list): List of Document objects\n        embeddings (Embeddings): Embedding model\n        collection_name (str): Name of the Chroma collection\n    \n    Returns:\n        Chroma: Populated Chroma vector store\n    \"\"\"\n    # Add documents to the vector store\n    retriever.add_documents(documents)\n    \n    return retriever\n\n\nfile_path = \"/kaggle/input/rag-documents/About UIU.txt\"\n\n# Read the text file\ntext = read_txt_file(file_path)\n\nif not text:\n    print(f'No text is found!')\nelse:\n    # Create text splitter\n    text_splitter = create_text_splitter()\n    \n    # Create documents\n    documents = create_documents(text, text_splitter)\n    \n    \n    # Ingest to Chroma vector store\n    retriever = ingest_to_chroma(documents, embedding)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:32:23.504813Z","iopub.execute_input":"2025-02-25T07:32:23.505166Z","iopub.status.idle":"2025-02-25T07:32:23.715731Z","shell.execute_reply.started":"2025-02-25T07:32:23.505131Z","shell.execute_reply":"2025-02-25T07:32:23.714951Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"query = \"Tell me the VC\"\ndocs = retriever.similarity_search_with_relevance_scores(\n                        query=query\n                    )\ncontent, _= docs[0]\nprint(content.page_content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T08:16:04.341610Z","iopub.execute_input":"2025-02-25T08:16:04.341925Z","iopub.status.idle":"2025-02-25T08:16:04.414086Z","shell.execute_reply.started":"2025-02-25T08:16:04.341898Z","shell.execute_reply":"2025-02-25T08:16:04.413318Z"}},"outputs":[{"name":"stdout","text":"Computer Laboratory:\n- Software Engineering Laboratory\n- Network Laboratory\n- Multimedia Laboratory\n- Hardware Laboratory\n\n\nLibrary and Documentation Center:\nUIU Central library has a collection of 40,293 items of information materials. Among the materials, 86,200 and 12,458 are books and bound periodicals respectively. Besides, 141 titles are in the current subscription list of journals. Every year, 500 volumes are added to the main reading room of the central library.\n\n\nResearch Center:\n- Center for Energy Research (CER)\n- Biomedical Engineering Center\n- Center for Emerging Networks and Technologies Research (CENTeR)\n- Brain-Computer Interface(BCI) Research Lab\n\n\nEvents:\n- International Career Summit\n- Photography Festival\n- BANMUN\n- Tech Quest '18\n- Tech Quest '16\n\n\nList of vice-chancellors:\n- Md. Abul Kashem Mia\n","output_type":"stream"}],"execution_count":38},{"cell_type":"markdown","source":"## Prompts","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\n\ndef get_chat_prompt(user_input, history, context=None):\n    today_date = datetime.today().strftime(\"%d %B %Y\")  # Dynamic date insertion\n\n    prompt = (\n        \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n\"\n        f\"Cutting Knowledge Date: December 2023\\n\"\n        f\"Today Date: {today_date}\\n\\n\"\n        \"You are a helpful assistant. DO NOT provide information which is not present on the Retrieved Context.\\n\"\n    )\n\n    # Add retrieved context if available\n    if context:\n        prompt += \"\\nRetrieved Context:\\n\" + context + \"\\n\"\n\n    prompt += \"<|eot_id|>\"\n\n    # Append chat history\n    for role, message in history:\n        prompt += f\"<|start_header_id|>{role}<|end_header_id|>\\n\\n{message}<|eot_id|>\"\n\n    # Append current user input\n    prompt += f\"<|start_header_id|>user<|end_header_id|>\\n\\n{user_input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n\n    return prompt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T08:26:42.599588Z","iopub.execute_input":"2025-02-25T08:26:42.599909Z","iopub.status.idle":"2025-02-25T08:26:42.604567Z","shell.execute_reply.started":"2025-02-25T08:26:42.599886Z","shell.execute_reply":"2025-02-25T08:26:42.603564Z"}},"outputs":[],"execution_count":41},{"cell_type":"markdown","source":"## Chain","metadata":{}},{"cell_type":"code","source":"history = []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:22:49.267355Z","iopub.status.idle":"2025-02-25T07:22:49.267783Z","shell.execute_reply":"2025-02-25T07:22:49.267604Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Conversation","metadata":{}},{"cell_type":"code","source":"while True:\n    user_input = input(\"Enter your message: \").strip()\n    if user_input.lower() == \"exit\":\n        break\n\n    prompt = get_chat_prompt(user_input, history)\n    response = generate(prompt)\n\n    print(f'User: \\n{user_input} \\nAssistant:\\n{response}\\n', flush=True)\n    print('-'*100, flush=True)\n    \n    history.append((\"user\", user_input))\n    history.append((\"assistant\", response))\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T07:22:49.268657Z","iopub.status.idle":"2025-02-25T07:22:49.269041Z","shell.execute_reply":"2025-02-25T07:22:49.268876Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Gradio","metadata":{}},{"cell_type":"code","source":"class ChatSystem:\n    def __init__(self, llm, retriever):\n        self.history = []\n        self.llm = llm\n        self.retriever = retriever\n\n    def retrieve(self, query):\n        docs = self.retriever.similarity_search_with_relevance_scores(query=query)\n        \n        context = \"\"\n        for doc in docs:\n            content, _ = doc\n            context += content.page_content\n        return context\n\n    \n    def chat(self, user_input):\n        context = self.retrieve(user_input)\n        prompt = get_chat_prompt(user_input, self.history, context)\n        response = generate(prompt)\n    \n        print(f'User: \\n{user_input} \\nAssistant:\\n{response}\\n', flush=True)\n        print('-'*100, flush=True)\n        \n        self.history.append((\"user\", user_input))\n        self.history.append((\"assistant\", response))\n        return response","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T08:31:00.138059Z","iopub.execute_input":"2025-02-25T08:31:00.138417Z","iopub.status.idle":"2025-02-25T08:31:00.144486Z","shell.execute_reply.started":"2025-02-25T08:31:00.138373Z","shell.execute_reply":"2025-02-25T08:31:00.143612Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"chatsystem = ChatSystem(llm, retriever)\n\nmessage = \"Tell about the vc of UIU\"\nchatsystem.chat(message)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T08:31:02.902882Z","iopub.execute_input":"2025-02-25T08:31:02.903176Z","iopub.status.idle":"2025-02-25T08:31:05.851140Z","shell.execute_reply.started":"2025-02-25T08:31:02.903154Z","shell.execute_reply":"2025-02-25T08:31:05.850478Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:1129: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\n\nllama_print_timings:        load time =    1103.21 ms\nllama_print_timings:      sample time =      43.33 ms /    22 runs   (    1.97 ms per token,   507.68 tokens per second)\nllama_print_timings: prompt eval time =    2194.95 ms /  1005 tokens (    2.18 ms per token,   457.87 tokens per second)\nllama_print_timings:        eval time =     631.21 ms /    21 runs   (   30.06 ms per token,    33.27 tokens per second)\nllama_print_timings:       total time =    2898.11 ms /  1026 tokens\n","output_type":"stream"},{"name":"stdout","text":"User: \nTell about the vc of UIU \nAssistant:\nThe Vice-Chancellor of United International University (UIU) is Md. Abul Kashem Mia.\n\n----------------------------------------------------------------------------------------------------\n<__main__.ChatSystem object at 0x795e002ca3b0>\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"chatsystem = ChatSystem(llm, retriever)\n\ndef chatbot(message: str, history: List[Tuple[str, str]]) -> List[Tuple[str, str]]:\n    \"\"\"Main chatbot function that processes messages and maintains conversation history\"\"\"\n\n    # Get response from API\n    bot_response = chatsystem.chat(message)\n\n    # Return the updated history\n    history = history or []\n    history.append((message, bot_response))\n\n    return history\n\n\n# Create the Gradio interface\nwith gr.Blocks(css=\"footer {visibility: hidden}\") as demo:\n    gr.Markdown(\"# 🤖 Rokomari Chatbot\")\n    gr.Markdown(\"Chat with this LLM-powered bot! Ask any question and get an intelligent response.\")\n    gr.Markdown(\"Powered by Nascenia\")\n\n    chatbot_interface = gr.Chatbot(\n        label=\"Chat History\",\n        height=600\n    )\n\n    msg = gr.Textbox(\n        label=\"Type your message\",\n        placeholder=\"Type your message here...\",\n        lines=1\n    )\n\n    clear = gr.Button(\"Clear Chat\")\n\n\n    def user_input(message, history):\n        history = history or []\n        return \"\", chatbot(message, history)\n\n\n    def clear_chat():        \n        return None, None\n\n\n    msg.submit(\n        user_input,\n        [msg, chatbot_interface],\n        [msg, chatbot_interface]\n    )\n\n    clear.click(\n        clear_chat,\n        None,\n        [msg, chatbot_interface]\n    )\n\nif __name__ == \"__main__\":\n    \n    demo.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-25T08:31:19.601252Z","iopub.execute_input":"2025-02-25T08:31:19.601617Z","iopub.status.idle":"2025-02-25T08:31:22.504241Z","shell.execute_reply.started":"2025-02-25T08:31:19.601588Z","shell.execute_reply":"2025-02-25T08:31:22.503550Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Running on local URL:  http://127.0.0.1:7861\nRunning on public URL: https://d2067b5706933fcc13.gradio.live\n\nThis share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://d2067b5706933fcc13.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:1129: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1103.21 ms\nllama_print_timings:      sample time =      36.68 ms /    20 runs   (    1.83 ms per token,   545.33 tokens per second)\nllama_print_timings: prompt eval time =    1735.70 ms /   748 tokens (    2.32 ms per token,   430.95 tokens per second)\nllama_print_timings:        eval time =     545.68 ms /    19 runs   (   28.72 ms per token,    34.82 tokens per second)\nllama_print_timings:       total time =    2345.52 ms /   767 tokens\n","output_type":"stream"},{"name":"stdout","text":"User: \nWho is the VC? \nAssistant:\nThe Vice-Chancellor (VC) of UIU is Md. Abul Kashem Mia.\n\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:1129: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1103.21 ms\nllama_print_timings:      sample time =      49.59 ms /    26 runs   (    1.91 ms per token,   524.33 tokens per second)\nllama_print_timings: prompt eval time =     617.19 ms /   188 tokens (    3.28 ms per token,   304.61 tokens per second)\nllama_print_timings:        eval time =     717.28 ms /    25 runs   (   28.69 ms per token,    34.85 tokens per second)\nllama_print_timings:       total time =    1417.46 ms /   213 tokens\n","output_type":"stream"},{"name":"stdout","text":"User: \nTell me something about him  \nAssistant:\nI do not have any information about Md. Abul Kashem Mia, apart from being the Vice Chancellor of UIU.\n\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:1129: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1103.21 ms\nllama_print_timings:      sample time =     383.04 ms /   192 runs   (    1.99 ms per token,   501.26 tokens per second)\nllama_print_timings: prompt eval time =    2268.23 ms /  1030 tokens (    2.20 ms per token,   454.10 tokens per second)\nllama_print_timings:        eval time =    5713.48 ms /   191 runs   (   29.91 ms per token,    33.43 tokens per second)\nllama_print_timings:       total time =    8626.82 ms /  1221 tokens\n","output_type":"stream"},{"name":"stdout","text":"User: \nTell me about UIU  \nAssistant:\nUnited International University (UIU) is a private research university in Dhaka, Bangladesh.\n\nHere are some key points about UIU:\n\n1. **Established**: UIU was established in 2003.\n2. **Location**: The permanent campus of UIU is located at the 'United City' at Satarkul, Badda (Dhaka).\n3. **Accreditation**: UIU is accredited by the Institution of Engineers, Bangladesh and ACBSP.\n4. **School types**: UIU is a private university that offers co-education.\n5. **Motto**: The motto of UIU is \"Quest for Excellence\".\n6. **Affiliations**: UIU is affiliated with the University Grants Commission (UGC).\n7. **Ranking**: According to the QS World University Rankings, UIU is ranked in 1201-1400 globally.\n\nThese are some key points about United International University (UIU).\n\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:1129: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1103.21 ms\nllama_print_timings:      sample time =     356.64 ms /   182 runs   (    1.96 ms per token,   510.32 tokens per second)\nllama_print_timings: prompt eval time =    2299.11 ms /  1032 tokens (    2.23 ms per token,   448.87 tokens per second)\nllama_print_timings:        eval time =    5411.81 ms /   181 runs   (   29.90 ms per token,    33.45 tokens per second)\nllama_print_timings:       total time =    8294.65 ms /  1213 tokens\n","output_type":"stream"},{"name":"stdout","text":"User: \nexplain point 2 in details. \nAssistant:\n**Location of United International University (UIU)**\n\nThe permanent campus of UIU is located at the 'United City' at Satarkul, Badda (Dhaka).\n\nHere are some key points about the location:\n\n1. **Satarkul**: The university's campus is situated in Satarkul, a locality in Dhaka.\n2. **Badda**: Satarkul is part of the Badda area, which is located in the northern part of Dhaka.\n3. **Dhaka**: The city of Dhaka is the capital and largest city of Bangladesh. It is a major economic, cultural, and educational hub in South Asia.\n\nThe location of UIU's campus at Satarkul, Badda (Dhaka) provides easy access to various parts of the city, making it an ideal location for students, faculty members, and staff.\n\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/llama_cpp/llama.py:1129: RuntimeWarning: Detected duplicate leading \"<|begin_of_text|>\" in prompt, this will likely reduce response quality, consider removing it...\n  warnings.warn(\nLlama.generate: prefix-match hit\n\nllama_print_timings:        load time =    1103.21 ms\nllama_print_timings:      sample time =      45.29 ms /    22 runs   (    2.06 ms per token,   485.79 tokens per second)\nllama_print_timings: prompt eval time =    3304.53 ms /  1417 tokens (    2.33 ms per token,   428.81 tokens per second)\nllama_print_timings:        eval time =     648.61 ms /    21 runs   (   30.89 ms per token,    32.38 tokens per second)\nllama_print_timings:       total time =    4027.85 ms /  1438 tokens\n","output_type":"stream"},{"name":"stdout","text":"User: \nwho established this varsity? \nAssistant:\nUnited International University (UIU) was established with the generous support and patronage of the United Group.\n\n----------------------------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":48}]}